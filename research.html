<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Calarina Muslimani - Research</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <nav>
        <a href="index.html">About</a>
        <a href="research.html">Research</a>
        <a href="advocacy.html">Advocacy</a>
        <a href="cv.html">CV</a>
        <a href="index.html#contact">Contact</a>
    </nav>

    <main>
        <h1>Research</h1>
        
        <section class="publication">
            <h2>Current Research: Using Alignment to Improve Reward Design in RL</h2>
            <div class="research-item">
                <h3><a href="https://sites.google.com/ualberta.ca/towardsimprovingrewarddesign/home" target="_blank">Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners </a></h3>
                <p> In this work, we develop a reward alignment metric, the Trajectory Alignment Coefficient, to evaluate how well a reward function, discount factor pair encodes the preferences of a domain expert. The Trajectory Alignment Coefficient quantifies the similarity between a human stakeholder’s ranking of trajectory distributions and those induced by a given reward function, discount factor pair. 
                    We prove that this metric is invariant to potential based reward shaping and positive linear transformations. Additionally, we demostrate in a user study of RL practioneries, that access to our reward metric during reward selection can make the process easier and lead to more performant reward functions./</p>
                <p>Published at ICLR 2025.</p>
            </div>

        </section>
        <section class="publication">
            <h2>Past Research: Improving Preference-based RL Algorithms</h2>
            <div class="research-item">
                <h3><a href="https://sites.google.com/ualberta.ca/sub-optimal-data-pretraining/home" target="_blank">Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning</a></h3>
                <p>In this work, we propose a simple technique that utilizes reward-free, low-quality data to boost the performance of off-the-shelf preference-based RL algorithms. Most importantly, we validate our approach with a human-user study. </p>
                <p>Published at ICLR 2025.</p>
            </div>
            <div class="research-item">
                <h3 class="title-link"><a href="https://arxiv.org/pdf/2406.06495" target="_blank">Boosting Robustness in Preference-Based Reinforcement Learning with Dynamic Sparsity</a></h3>
                <p>In this paper, we tackle the problem of learning reward models from human preferences in extremely noisy environments. We found that by leveraging principles of dynamic sparse training, reward models can effectively learn to focus on task-relevant features.</p>
                <p>Published at AAMAS 2025 (Extended Abstract).</p>
            </div>
        </section>
        <section class="publication">
            <h2>Publications</h2>
            <div class="research-item">
                <h3>You can find my articles on my <a href="https://scholar.google.com/citations?user=3S4LYDQAAAAJ&hl=en" target="_blank">Google Scholar</a> profile.</h3>
            </div>
        </section>
    </main>

    <footer>
        <p>© 2025 Calarina Muslimani. All rights reserved.</p>
    </footer>
</body>
</html>